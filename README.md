# FastMovieMaker

> ğŸ¬ Professional Video Subtitle Editor with AI-Powered Transcription

**FastMovieMaker** is a desktop application for creating, editing, and exporting video subtitles with advanced features like multi-source video editing, automatic transcription via Whisper, and AI-powered text-to-speech.

[![Python](https://img.shields.io/badge/Python-3.13-blue.svg)](https://www.python.org/)
[![PySide6](https://img.shields.io/badge/PySide6-6.10-green.svg)](https://pypi.org/project/PySide6/)
[![Tests](https://img.shields.io/badge/tests-43%20passing-brightgreen.svg)](tests/)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](LICENSE)

---

## âœ¨ Key Features

### ğŸ¯ AI-Powered Subtitle Generation
- **Faster-Whisper Integration** â€” Optimized speech recognition with CTranslate2 (up to 4x faster)
- Support for multiple Whisper models (tiny, base, small, medium, large)
- Real-time transcription progress with cancel support

### ğŸï¸ Multi-Source Video Editing
- **Advanced Timeline** â€” Combine clips from different video files (Aâ†’Bâ†’A patterns)
- Frame-accurate editing with custom QPainter timeline widget
- Seamless clip boundary transitions with automatic source switching
- **43 comprehensive unit tests** ensuring rock-solid multi-source playback
- **Smart Aspect Ratio Adaptation** â€” Subtitles automatically adjust layout for 9:16 (Shorts/Reels) templates

### ğŸ¨ Professional Video Preview
- **Frame Cache System** â€” Instant scrub preview with FFmpeg-extracted frames
- Real-time subtitle overlay with customizable styles
- Image overlay support (PIP) with position/scale controls
- Dark theme UI with QSS styling

### ğŸ”Š AI Text-to-Speech
- **Multiple TTS Engines:**
  - Edge-TTS (Microsoft Azure voices)
  - ElevenLabs API integration
- Per-segment TTS generation and audio mixing
- Independent volume controls for video and TTS audio

### ğŸŒ Internationalization
- **Full i18n Support** â€” Korean (í•œêµ­ì–´) and English
- Locale-aware UI with runtime language switching
- Comprehensive translation coverage

### ğŸ“¦ Export & Import
- **Flexible Export:**
  - SRT subtitle files
  - Batch video rendering with subtitles burned-in
  - Custom resolution presets (1080p, 720p, 480p)
- **Project Management:**
  - Save/load `.fmm.json` project files
  - Auto-save with backup system
  - Undo/redo support with QUndoStack

---

## ğŸ—ï¸ Architecture

### Clean 3-Layer Design
```
src/
â”œâ”€â”€ models/          # Pure Python data models (Qt-independent)
â”‚   â”œâ”€â”€ project.py
â”‚   â”œâ”€â”€ subtitle.py
â”‚   â”œâ”€â”€ video_clip.py
â”‚   â””â”€â”€ style.py
â”œâ”€â”€ services/        # Business logic (FFmpeg, Whisper, TTS)
â”‚   â”œâ”€â”€ ffmpeg_service.py
â”‚   â”œâ”€â”€ whisper_service.py
â”‚   â”œâ”€â”€ tts_service.py
â”‚   â””â”€â”€ frame_cache_service.py
â”œâ”€â”€ workers/         # QThread background workers
â”‚   â”œâ”€â”€ whisper_worker.py
â”‚   â”œâ”€â”€ tts_worker.py
â”‚   â”œâ”€â”€ waveform_worker.py
â”‚   â””â”€â”€ frame_cache_worker.py
â””â”€â”€ ui/              # PySide6 UI components
    â”œâ”€â”€ main_window.py
    â”œâ”€â”€ timeline_widget.py
    â”œâ”€â”€ video_player_widget.py
    â””â”€â”€ playback_controls.py
```

### Technical Highlights
- **Worker-moveToThread Pattern** â€” Non-blocking background processing for Whisper/TTS
- **Custom QPainter Timeline** â€” Frame-accurate video editing with zoom/scroll
- **Multi-Source Playback System:**
  - Explicit `_current_clip_index` tracking (no ambiguous sourceâ†’timeline mapping)
  - Clip boundary detection (30ms threshold) for auto-transition
  - Frame cache integration for instant scrub preview
- **Output Time Mode** â€” Unified timelineâ†’slider synchronization across Aâ†’Bâ†’A clips

---

## ğŸš€ Installation

### Requirements
- **Python 3.13+** (3.9+ supported with `from __future__ import annotations`)
- **FFmpeg** (required for video processing)
- **NVIDIA GPU** (optional, for CUDA-accelerated Whisper)

### Setup
```bash
# Clone repository
git clone https://github.com/yourusername/FastMovieMaker.git
cd FastMovieMaker

# Create virtual environment
python -m venv .venv
.venv\Scripts\activate  # Windows
# source .venv/bin/activate  # Linux/Mac

# Install dependencies
pip install -r requirements.txt

# Install PyTorch with CUDA support (optional, for GPU acceleration)
pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu124

# Run application
python main.py
```

### FFmpeg Installation
- **Windows:** Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add to PATH
- **Linux:** `sudo apt install ffmpeg`
- **Mac:** `brew install ffmpeg`

---

## ğŸ§ª Testing

### Comprehensive Test Suite
```bash
# Run all tests (326+ test cases across 20 modules)
pytest tests/ -v

# Run multi-source playback tests (43 test cases)
pytest tests/test_multi_source_playback.py -v

# Test categories:
# - Scrub source switching
# - Play/pause race conditions
# - Media status handling
# - Position changed events
# - Scrubâ†’play scenarios
# - Play button sync
# - Clip boundary crossing
# - Timeline/slider sync
# - Edge cases (short clips, rapid transitions, etc.)
```

---

## ğŸ› ï¸ Tech Stack

| Category | Technology |
|----------|-----------|
| **Language** | Python 3.13 |
| **GUI Framework** | PySide6 6.10 (Qt 6.10) |
| **Video Processing** | FFmpeg, opencv-python |
| **AI/ML** | OpenAI Whisper, PyTorch 2.6 (CUDA 12.4) |
| **TTS** | Edge-TTS, ElevenLabs API |
| **Testing** | pytest, pytest-qt |
| **I18n** | Custom translation system |

---

## ğŸ“– Usage

### Basic Workflow
1. **Load Video** â€” Drag & drop or File â†’ Open Video
2. **Generate Subtitles:**
   - Option A: Subtitle â†’ Generate from Whisper
   - Option B: Subtitle â†’ Generate from Script (TTS)
3. **Edit Timeline:**
   - Add video clips from different sources
   - Adjust subtitle timing by dragging segments
   - Edit text in the subtitle table
4. **Export:**
   - File â†’ Export â†’ SRT File
   - File â†’ Export â†’ Batch Export (burned-in subtitles)

### ğŸ“š Detailed Guides
- **[TTS Usage Guide (í•œêµ­ì–´)](docs/TTS_USAGE.md)** â€” í…ìŠ¤íŠ¸ ìŒì„± ë³€í™˜ ìƒì„¸ ê°€ì´ë“œ
- **[TTS Usage Guide (English)](docs/TTS_USAGE_EN.md)** â€” Comprehensive TTS tutorial

### Multi-Source Video Editing
```python
# Example: A(0-10s) â†’ B(0-5s) â†’ A(10-20s) timeline
from src.models.video_clip import VideoClip, VideoClipTrack

track = VideoClipTrack(clips=[
    VideoClip(0, 10000),               # A: 0-10s
    VideoClip(0, 5000),                # B: 0-5s (external source)
    VideoClip(10000, 20000),           # A: 10-20s
])
track.clips[1].source_path = "path/to/video_b.mp4"

# Total output duration: 25 seconds (10 + 5 + 10)
```

---

## ğŸ¯ Roadmap

- [ ] Real-time subtitle preview during Whisper transcription
- [ ] GPU-accelerated video rendering
- [ ] Plugin system for custom TTS providers
- [ ] Collaborative editing (cloud project sync)
- [ ] Subtitle translation with AI (DeepL/GPT integration)

---

## ğŸ“ License

MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- [OpenAI Whisper](https://github.com/openai/whisper) â€” Speech recognition model
- [PySide6](https://pypi.org/project/PySide6/) â€” Qt for Python
- [FFmpeg](https://ffmpeg.org/) â€” Video processing
- [Edge-TTS](https://github.com/rany2/edge-tts) â€” Microsoft Azure TTS

---

## ğŸ¤ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Development Setup
```bash
# Install dev dependencies
pip install pytest pytest-qt black ruff

# Run tests before committing
pytest tests/ -v

# Format code
black src/ tests/
ruff check src/ tests/
```

---

## ğŸ’¬ Contact

- **Issues:** [GitHub Issues](https://github.com/yourusername/FastMovieMaker/issues)
- **Discussions:** [GitHub Discussions](https://github.com/yourusername/FastMovieMaker/discussions)

---

<div align="center">
Made with â¤ï¸ by [Your Name]
</div>
